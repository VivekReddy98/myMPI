
Author:
#vkarri Vivek Reddy Karri

Commands to Execute my_rtt.c program using the functions written in my_mpi library:
1) To compile: make -f p1.Makefile
2) To execute the program: ./my_prun.sh #{number of procs} ./my_rtt

Files and their purpose of existence:
1) my_mpi.c/my_mpi_helper.c: Source Files which has function definitions used in my_mpi library. my_mpi.c has core MPI functions and other functions are defined in my_mpi_helper.c
2) my_mpi.h: Constants, Function, Structs are defined in here.
3) my_prun.sh: A Bash script to spawn the executable across the nodes and managing them through SSH.
4) my_rtt.c: Program which measure round trip time among the nodes.
5) nodefile.txt: Text file generated by the bash script which is used by the executable using the my_mpi library across the nodes to get the Ip addresses on all the nodes in the game.

Implementational Details:
1) Although, File System is shared across the nodes, it is not used for Synchronization or any sort of communication across the nodes.
(Only used to read nodefile.txt in the beggining to get the names of the nodes followed by Ip addresses)
2) Server-Client Architecture is used for communication. Server is only for Reading the Messages and only Client Will send the Messages.
3) Sever handles multiple connections using select() and its related functions.
34 Mutex Locks and Conditional Variables are used for synchronization.

Functions and their Details:
1) MPI_Init(): This function is used to establish communication across all the nodes.
   Essentially:  1) It creates a server thread and listens from connections from all the nodes. (Every Node does this)
                 2) So finally a fully connected network is formed across all the nodes, with every node maintaining two seperate arrays one for client Sockets and server Sockets indexed by Rank.
2) MPI_Finalize(): This will closes all the sockets and does after after letting every node reaching the Barrier.
3) MPI_Barrier(): This will make sure every node reaches this point in the execution before going further. Does this by sending a message to root node and waiting for a ACK message from the same.
                Root node on the other hand waits to recieve a message from all the sends and only then sending Acknowledgement to all the other nodes.
4) MPI_Sendrecv(): This will span a new thread, which listens for incoming message from the source specified in the argument.
                  In paralell, Client starts sending the message to destination specified in the arguments.
                  Client Resumes the execution until a wake up signal is sent from server thread that it has recieved a message from Source .
                  Then client send a Acknowledgement message to the other node and resumes execution again until the ack message sent by the other node is recieved by this node.
                  Synchronization is recieved because this is supposed to be a Blocking command.


Results from Execution of my_rtt from two nodes: (Custom Implementation)

(Format: Rank, Msg Size in Bytes, Mean of the rtt(in micro Sec), st deviation of the rtt(micro Sec))

1 16 379.400000 111.891769
0 16 361.600000 100.979840
1 32 348.200000 92.155911
0 32 339.000000 93.657888
1 64 340.400000 81.325322
0 64 353.000000 80.814603
1 128 367.000000 83.965469
0 128 349.400000 84.456900
0 256 269.000000 69.069530
1 256 278.000000 66.507142
1 512 298.600000 67.161358
0 512 314.600000 73.756410
1 1024 314.000000 71.102743
0 1024 312.200000 73.342975
1 2048 305.200000 75.516568
0 2048 313.800000 72.801593
0 4096 417.200000 132.584283
1 4096 449.800000 149.891134
0 8192 324.400000 110.139766
1 8192 362.200000 143.494920
0 16384 293.000000 67.966168
1 16384 304.400000 112.062340
0 32768 313.800000 149.730665
1 32768 304.000000 69.059395
0 65536 419.800000 126.862729
1 65536 378.600000 130.971172
0 131072 25366.800000 17109.904877
1 131072 25273.000000 17046.954397
0 262144 46744.400000 40299.327562
1 262144 46883.200000 48740.290370
1 524288 42758.200000 40179.810188
0 524288 42957.000000 40270.226687
1 1048576 52561.200000 33850.886551
0 1048576 36061.000000 8083.930640
0 2097152 80815.800000 24332.081658
1 2097152 81370.800000 44568.419586

Results from Execution of my_rtt from two nodes: (Commercial Implementation of MPI using MPICC)

(Format: Rank, Msg Size in Bytes, Mean of the rtt(in micro Sec), st deviation of the rtt(micro Sec))

1 16 31.700000 75.293565
1 32 4.500000 3.636619
0 16 33.200000 64.267690
1 64 6.600000 8.502000
0 32 3.600000 2.396664
1 128 3.900000 1.723079
0 64 3.200000 1.317574
1 256 5.800000 4.538282
0 128 3.900000 1.330038
1 512 5.800000 3.794206
0 256 5.200000 3.981959
1 1024 10.900000 15.537342
0 512 5.900000 4.172409
1 2048 15.700000 21.179259
0 1024 12.800000 16.437031
1 4096 11.700000 6.318307
0 2048 16.100000 20.089027
1 8192 22.100000 9.922147
0 4096 13.600000 7.024528
1 16384 35.100000 20.285192
0 8192 21.000000 8.769265
0 16384 31.500000 9.467048
1 32768 43.200000 15.353697
0 32768 43.200000 14.674331
1 65536 48.000000 9.797959
0 65536 48.000000 9.528903
1 131072 81.700000 9.236937
0 131072 81.900000 9.269790
1 262144 151.000000 16.046807
0 262144 150.300000 15.924855
1 524288 287.700000 30.362493
0 524288 288.100000 30.445509
1 1048576 564.200000 59.492823
0 1048576 564.200000 59.501227
1 2097152 1122.800000 118.724791
0 2097152 1122.400000 118.645961

Observations:
1) For one thing, A huuuuuge scope of Optimization for sure.
2) Also, there is continous growth of rtt for the growing message sizes, which is expected.
3) However, there is an anomalous increase in the rtt between 65KB to 0.13 MB, which then linearly scaled continued thereafter.
